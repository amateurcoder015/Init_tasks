1. Paper Review:  
   This paper describes how a deep convolutional neural network (later known as “AlexNet”) was trained to classify 1.2 million images from ImageNet into 1000 categories. Previous computer vision methods used hand-crafted features like SIFT, but they didn’t scale well to such large image variation. The authors built a large 8-layer CNN (5 convolution \+ 3 fully-connected) with \~60 million parameters and trained it end-to-end on raw RGB pixels. They introduced several ideas which were new or uncommon at that time: ReLU activation for faster training, overlapping pooling, training across two GPUs, heavy data augmentation, and dropout to reduce overfitting. On the ImageNet 2012 challenge, their model had an enormous improvement: top-5 error 15.3% vs 26.2% from the second best approach. This result basically proved that deep learning \+ big GPUs \+ big data is better than hand-engineered features.  
     
2. Self Reflection:   
   The major limitation I personally see in this paper is that their success depended heavily on massive compute (2 GPUs for 6 days) and the availability of extremely large labelled datasets. If someone tried this idea in 2010 when ImageNet wasn't public, it would have failed-not because neural nets were bad, but because there wasn't enough data \+ compute. So in a way, the "innovation" here is partly scientific and partly timing. Another limitation is that the architecture search was manual-they hand-designed the number of layers, filters, and connections. Today, we have better techniques (automated architecture search, better regularization, better optimizers) that make model design less guess-based.  
     
   A possible future direction, later materialized, is a combination of convolution with even deeper architectures (like ResNets) using unsupervised/ self-supervised learning. Indeed, the authors mention in this paper that unsupervised pre-training could give an edge when labels are limited. In retrospect, this paper marked the turning point where vision moved from hand-crafting to learned representations-but the next big leap is learning from unlabelled data, not annotated datasets.  
   